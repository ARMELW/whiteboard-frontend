import React, { useRef, useState, useEffect } from "react";
import { Button, Card, CardContent, CardHeader, CardTitle } from './atoms';
import { Upload, Play, Download, FileJson } from "lucide-react";
import { ImageCropModal } from "./molecules";

/**
 * HandWritingFromPython.jsx
 * Transcription du script Python -> logique équivalente en JS (browser)
 *
 * Fonctionnalités:
 * - Upload image
 * - Prétraitement : resize, grayscale, seuillage (approx adaptatif)
 * - Prétraitement main : découpe par mask
 * - Découpage en grille, selection de cellules avec pixels noirs -> REMPLACÉ PAR SUIVI DE STROKES
 * - Dessin progressif en suivant cellule la plus proche -> REMPLACÉ PAR TRACÉ DE STROKES
 * - Enregistrement vidéo via MediaRecorder depuis le canvas
 * - **NEW**: Load and replay animation from JSON file (generated by Python script)
 */

const HandWritingAnimation = () => {
  const sourceCanvasRef = useRef(null); // preview source
  const drawCanvasRef = useRef(null); // drawing canvas (recorded)
  const mediaRecorderRef = useRef(null);
  const recordedBlobsRef = useRef([]);

  const [sourceImage, setSourceImage] = useState(null);
  const [handImage, setHandImage] = useState(null);
  const [handMask, setHandMask] = useState(null);
  const [isGenerating, setIsGenerating] = useState(false);
  const [progress, setProgress] = useState(0);
  const [videoUrl, setVideoUrl] = useState(null);
  
  // New state for JSON animation mode
  const [mode, setMode] = useState("image"); // "image" or "json"
  const [animationData, setAnimationData] = useState(null);
  const [sourceImageForJson, setSourceImageForJson] = useState(null);
  
  // State for image cropping
  const [showCropModal, setShowCropModal] = useState(false);
  const [pendingImageUrl, setPendingImageUrl] = useState(null);
  const [cropTargetMode, setCropTargetMode] = useState(null); // "image" or "json"

  // default variables mirroring AllVariables in python
  const variablesDefault = {
    frameRate: 30,
    resizeWd: 640,
    resizeHt: 360,
    // splitLen: 10, // Plus utilisé pour le tracé de strokes
    strokeTraceSpeed: 3, // Nombre de pixels tracés par "étape" d'animation
    endGrayImgDurationInSec: 2,
  };

  // load hand images from public/data/images on mount
  useEffect(() => {
    const hand = new Image();
    hand.src = "/data/images/drawing-hand.png";
    hand.onload = () => setHandImage(hand);

    const mask = new Image();
    mask.src = "/data/images/hand-mask.png";
    mask.onload = () => setHandMask(mask);
  }, []);

  /* -----------------------------
     Utility functions (browser)
     ----------------------------- */

  // Supprimé: eucDistArr, blockHasBlack (plus pertinent pour la logique de cellules)
  // applyThreshold (plus pertinent car on utilise adaptiveThresholdApprox)

  // resize image to target dims and return ImageData
  const resizeAndGetImageData = (img, w, h) => {
    const canvas = document.createElement("canvas");
    canvas.width = w;
    canvas.height = h;
    const ctx = canvas.getContext("2d");
    ctx.drawImage(img, 0, 0, w, h);
    return ctx.getImageData(0, 0, w, h);
  };

  // convert ImageData to grayscale in place
  const toGrayscale = (imageData) => {
    const d = imageData.data;
    for (let i = 0; i < d.length; i += 4) {
      const r = d[i],
        g = d[i + 1],
        b = d[i + 2];
      const gray = 0.299 * r + 0.587 * g + 0.114 * b;
      d[i] = d[i + 1] = d[i + 2] = gray;
    }
    return imageData;
  };

  // Apply edge detection to enhance contours
  const applyEdgeDetection = (imageData) => {
    const w = imageData.width;
    const h = imageData.height;
    const d = imageData.data;
    
    // Create a copy for edge detection
    const gray = new Uint8ClampedArray(w * h);
    for (let i = 0; i < w * h; i++) {
      gray[i] = d[i * 4];
    }
    
    const edges = new Uint8ClampedArray(w * h);
    
    // Sobel operator kernels
    const sobelX = [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]];
    const sobelY = [[-1, -2, -1], [0, 0, 0], [1, 2, 1]];
    
    for (let y = 1; y < h - 1; y++) {
      for (let x = 1; x < w - 1; x++) {
        let gx = 0;
        let gy = 0;
        
        // Apply Sobel kernels
        for (let ky = -1; ky <= 1; ky++) {
          for (let kx = -1; kx <= 1; kx++) {
            const pixelValue = gray[(y + ky) * w + (x + kx)];
            gx += pixelValue * sobelX[ky + 1][kx + 1];
            gy += pixelValue * sobelY[ky + 1][kx + 1];
          }
        }
        
        // Calculate gradient magnitude
        const magnitude = Math.sqrt(gx * gx + gy * gy);
        edges[y * w + x] = Math.min(255, magnitude);
      }
    }
    
    // Combine edges with original grayscale (weighted)
    for (let i = 0; i < w * h; i++) {
      const edgeStrength = edges[i] / 255;
      const originalValue = gray[i];
      // Enhance edges: darker where edges are strong
      const enhanced = Math.max(0, originalValue - edgeStrength * 100);
      const idx = i * 4;
      d[idx] = d[idx + 1] = d[idx + 2] = enhanced;
    }
    
    return imageData;
  };

  // Improved adaptive threshold using Gaussian-like weighted average
  // Mimics cv2.adaptiveThreshold with ADAPTIVE_THRESH_GAUSSIAN_C more closely
  const adaptiveThresholdApprox = (imageData, blockSize = 15, C = 10) => {
    // imageData will be modified in place
    const w = imageData.width;
    const h = imageData.height;
    const d = imageData.data;
    
    // Create a copy of grayscale values for calculation
    const gray = new Uint8ClampedArray(w * h);
    for (let i = 0; i < w * h; i++) {
      gray[i] = d[i * 4];
    }
    
    // Apply adaptive threshold pixel by pixel
    const halfBlock = Math.floor(blockSize / 2);
    
    for (let y = 0; y < h; y++) {
      for (let x = 0; x < w; x++) {
        const idx = y * w + x;
        const pixelValue = gray[idx];
        
        // Calculate local mean in the neighborhood
        let sum = 0;
        let count = 0;
        
        for (let dy = -halfBlock; dy <= halfBlock; dy++) {
          for (let dx = -halfBlock; dx <= halfBlock; dx++) {
            const ny = y + dy;
            const nx = x + dx;
            
            if (ny >= 0 && ny < h && nx >= 0 && nx < w) {
              // Apply Gaussian-like weighting (center pixels have more weight)
              const distance = Math.abs(dx) + Math.abs(dy);
              const weight = 1.0 / (1.0 + distance * 0.3);
              sum += gray[ny * w + nx] * weight;
              count += weight;
            }
          }
        }
        
        const localMean = count > 0 ? sum / count : 128;
        const threshold = localMean - C;
        
        // Apply threshold
        const outputValue = pixelValue < threshold ? 0 : 255;
        const outIdx = idx * 4;
        d[outIdx] = d[outIdx + 1] = d[outIdx + 2] = outputValue;
      }
    }
    
    return imageData;
  };

  // get extreme coordinates from a mask image
  const getExtremeCoordinatesFromMask = (maskImage) => {
    const canvas = document.createElement("canvas");
    canvas.width = maskImage.width;
    canvas.height = maskImage.height;
    const ctx = canvas.getContext("2d");
    ctx.drawImage(maskImage, 0, 0);
    const imgd = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const d = imgd.data;
    const w = canvas.width;
    const h = canvas.height;
    let minX = w,
      minY = h,
      maxX = 0,
      maxY = 0;
    for (let y = 0; y < h; y++) {
      for (let x = 0; x < w; x++) {
        const idx = (y * w + x) * 4;
        const v = d[idx];
        if (v > 128) { // Assuming white pixels indicate the mask
          if (x < minX) minX = x;
          if (y < minY) minY = y;
          if (x > maxX) maxX = x;
          if (y > maxY) maxY = y;
        }
      }
    }
    if (minX > maxX || minY > maxY) return null;
    return { minX, minY, maxX, maxY };
  };

  // preprocess hand: crop according to mask and create hand canvas & mask arrays
  const preprocessHand = async (handImg, handMaskImg) => {
    if (!handImg || !handMaskImg) return null;
    const bounds = getExtremeCoordinatesFromMask(handMaskImg);
    if (!bounds) return null;
    const w = bounds.maxX - bounds.minX + 1;
    const h = bounds.maxY - bounds.minY + 1;
    const canvas = document.createElement("canvas");
    canvas.width = w;
    canvas.height = h;
    const ctx = canvas.getContext("2d");
    ctx.drawImage(handImg, bounds.minX, bounds.minY, w, h, 0, 0, w, h);

    const maskCanvas = document.createElement("canvas");
    maskCanvas.width = w;
    maskCanvas.height = h;
    const mc = maskCanvas.getContext("2d");
    mc.drawImage(handMaskImg, bounds.minX, bounds.minY, w, h, 0, 0, w, h);
    const maskData = mc.getImageData(0, 0, w, h).data;
    const maskArr = new Uint8ClampedArray(w * h);
    
    // Apply transparency to the hand canvas based on mask
    const handImageData = ctx.getImageData(0, 0, w, h);
    const handData = handImageData.data;
    
    for (let i = 0; i < w * h; i++) {
      const v = maskData[i * 4];
      maskArr[i] = v > 128 ? 1 : 0; // 1 where hand (foreground), 0 where transparent (background)
      
      // Set alpha channel: transparent where mask is 0, opaque where mask is 1
      handData[i * 4 + 3] = maskArr[i] * 255;
    }
    
    // Put the modified image data back onto the canvas
    ctx.putImageData(handImageData, 0, 0);

    return {
      canvas,
      width: w,
      height: h,
      maskArr,
    };
  };

  /* -----------------------------
     Stroke detection and ordering logic
     ----------------------------- */

  // Fonction pour trouver et ordonner les strokes (chemins de pixels)
  const findAndOrderStrokes = (imageData, w, h) => {
    const d = imageData.data;
    const visited = new Uint8ClampedArray(w * h).fill(0); // 0: non visité, 1: visité
    const strokes = [];

    // Directions pour les 8 voisins (y, x)
    const directions = [
      [-1, -1], [-1, 0], [-1, 1],
      [0, -1],           [0, 1],
      [1, -1], [1, 0], [1, 1],
    ];

    // Simple fonction pour vérifier si un pixel est noir et non visité
    const isBlackAndUnvisited = (x, y) => {
      if (x < 0 || x >= w || y < 0 || y >= h) return false;
      const idx = (y * w + x) * 4;
      return d[idx] < 128 && visited[y * w + x] === 0; // d[idx] est la valeur de gris (after threshold: 0 or 255)
    };

    // Parcours en profondeur (DFS) pour trouver un stroke
    const dfs = (startX, startY) => {
      const currentStroke = [];
      const stack = [[startX, startY]];

      while (stack.length > 0) {
        const [cx, cy] = stack.pop(); // On prend le dernier ajouté
        const flatIdx = cy * w + cx;

        if (visited[flatIdx] === 1) continue;
        visited[flatIdx] = 1;
        currentStroke.push([cx, cy]);

        // Chercher les voisins noirs et non visités, en favorisant la continuité
        const neighbors = [];
        for (const [dy, dx] of directions) {
          const nx = cx + dx;
          const ny = cy + dy;
          if (isBlackAndUnvisited(nx, ny)) {
            neighbors.push([nx, ny]);
          }
        }

        // Pour un tracé plus "continu", trier les voisins par distance à la "direction précédente"
        // Ou simplement ajouter les voisins et laisser le stack gérer
        // Ici, on les ajoute simplement. La qualité du tracé dépendra de l'ordre d'ajout.
        // On pourrait affiner en choisissant le voisin qui continue le mieux la "direction"
        for(const neighbor of neighbors) {
          stack.push(neighbor);
        }
      }
      return currentStroke;
    };


    // Chercher tous les strokes
    for (let y = 0; y < h; y++) {
      for (let x = 0; x < w; x++) {
        const idx = (y * w + x) * 4;
        if (d[idx] < 128 && visited[y * w + x] === 0) { // Si pixel noir et non visité (threshold: 0 or 255)
          const newStroke = dfs(x, y);
          if (newStroke.length > 1) { // Un stroke doit avoir au moins 2 points
            strokes.push(newStroke);
          }
        }
      }
    }

    // Ici, nous pourrions ajouter une étape pour "ordonner" les strokes
    // par exemple, du haut vers le bas, ou de gauche à droite,
    // pour simuler un ordre d'écriture plus naturel.
    // Pour l'instant, ils sont dans l'ordre de découverte.
    // Un simple tri par le point de départ peut suffire pour commencer.
    strokes.sort((a, b) => {
      const [ax, ay] = a[0];
      const [bx, by] = b[0];
      if (ay !== by) return ay - by;
      return ax - bx;
    });

    return strokes;
  };

  /* -----------------------------
     JSON Animation Functions
     ----------------------------- */

  const handleJsonUpload = (e) => {
    const file = e.target.files[0];
    if (!file) return;

    const reader = new FileReader();
    reader.onload = (event) => {
      try {
        const jsonData = JSON.parse(event.target.result);
        
        // Validate JSON structure
        if (!jsonData.metadata || !jsonData.animation || !jsonData.animation.frames_written) {
          alert("Format JSON invalide. Le fichier doit contenir metadata et animation.frames_written");
          return;
        }

        setAnimationData(jsonData);
        setMode("json");
        console.log("JSON animation data loaded:", jsonData.metadata);
        
        // Show metadata preview
        alert(`Animation chargée:\n` +
              `Résolution: ${jsonData.metadata.width}x${jsonData.metadata.height}\n` +
              `FPS: ${jsonData.metadata.frame_rate}\n` +
              `Frames: ${jsonData.metadata.total_frames}`);
      } catch (error) {
        alert("Erreur lors de la lecture du fichier JSON: " + error.message);
      }
    };
    reader.readAsText(file);
  };

  const handleSourceImageForJsonUpload = (e) => {
    const file = e.target.files[0];
    if (!file) return;
    
    const reader = new FileReader();
    reader.onload = (event) => {
      setPendingImageUrl(event.target.result);
      setCropTargetMode("json");
      setShowCropModal(true);
    };
    reader.readAsDataURL(file);
    e.target.value = '';
  };

  const generateVideoFromJson = async (animData, sourceImg, handProc) => {
    const metadata = animData.metadata;
    const frames = animData.animation.frames_written;
    
    const w = metadata.width;
    const h = metadata.height;
    const fps = metadata.frame_rate;

    // Setup canvas
    const recordCanvas = drawCanvasRef.current;
    recordCanvas.width = w;
    recordCanvas.height = h;
    const recordCtx = recordCanvas.getContext("2d");
    
    // Initialize with white background
    recordCtx.fillStyle = "white";
    recordCtx.fillRect(0, 0, w, h);

    // Prepare source image resized to animation dimensions
    const sourceCanvas = document.createElement("canvas");
    sourceCanvas.width = w;
    sourceCanvas.height = h;
    const sourceCtx = sourceCanvas.getContext("2d");
    sourceCtx.drawImage(sourceImg, 0, 0, w, h);
    const sourceImageData = sourceCtx.getImageData(0, 0, w, h);

    // Convert to grayscale, edge detection and threshold for drawing
    const grayCanvas = document.createElement("canvas");
    grayCanvas.width = w;
    grayCanvas.height = h;
    const grayCtx = grayCanvas.getContext("2d");
    let grayImageData = sourceCtx.getImageData(0, 0, w, h);
    grayImageData = toGrayscale(grayImageData);
    grayImageData = applyEdgeDetection(grayImageData); // Add edge detection
    grayImageData = adaptiveThresholdApprox(grayImageData, 15, 10);
    grayCtx.putImageData(grayImageData, 0, 0);

    // Setup drawing canvas (what has been drawn so far)
    const drawnCanvas = document.createElement("canvas");
    drawnCanvas.width = w;
    drawnCanvas.height = h;
    const drawnCtx = drawnCanvas.getContext("2d");
    drawnCtx.fillStyle = "white";
    drawnCtx.fillRect(0, 0, w, h);

    // Setup media recorder
    recordedBlobsRef.current = [];
    setVideoUrl(null);

    const stream = recordCanvas.captureStream(fps);
    const mediaRecorder = new MediaRecorder(stream, { 
      mimeType: "video/webm;codecs=vp9",
      videoBitsPerSecond: 2500000 
    });
    
    mediaRecorder.ondataavailable = (ev) => {
      if (ev.data && ev.data.size > 0) {
        recordedBlobsRef.current.push(ev.data);
      }
    };
    
    mediaRecorder.start();
    mediaRecorderRef.current = mediaRecorder;

    // Replay animation from JSON frames
    for (let i = 0; i < frames.length; i++) {
      const frame = frames[i];
      const tileCoords = frame.tile_drawn.pixel_coords;
      const handPos = frame.hand_position;

      // Draw the tile from grayscale image to drawn canvas
      const tileData = grayCtx.getImageData(
        tileCoords.x_start,
        tileCoords.y_start,
        tileCoords.x_end - tileCoords.x_start,
        tileCoords.y_end - tileCoords.y_start
      );
      drawnCtx.putImageData(tileData, tileCoords.x_start, tileCoords.y_start);

      // Composite: drawn content
      recordCtx.clearRect(0, 0, w, h);
      recordCtx.drawImage(drawnCanvas, 0, 0);

      // Draw hand at position
      if (handProc && handProc.canvas) {
        const handW = handProc.width;
        const handH = handProc.height;
        // Center the hand on the drawing position (similar to Python behavior)
        const handX = handPos.x - handW / 2;
        const handY = handPos.y - handH / 2;
        
        // Only draw if hand is within bounds
        if (handX + handW > 0 && handX < w && handY + handH > 0 && handY < h) {
          // Draw the hand (now with proper transparency from preprocessHand)
          recordCtx.drawImage(handProc.canvas, handX, handY);
        }
      }

      // Update progress
      setProgress(Math.floor(((i + 1) / frames.length) * 90));

      // Wait for next frame
      await new Promise((r) => setTimeout(r, Math.max(0, 1000 / fps)));
    }

    // After all tiles are drawn, overlay the colored image on drawn areas
    // This matches Python behavior: variables.drawn_frame[object_ind] = variables.img[object_ind]
    const finalDrawnImageData = drawnCtx.getImageData(0, 0, w, h);
    const finalDrawnData = finalDrawnImageData.data;
    const grayData = grayImageData.data;
    const colorData = sourceImageData.data;
    
    // Replace black pixels (drawn areas) with original color
    for (let i = 0; i < w * h; i++) {
      const idx = i * 4;
      // If pixel is black (was drawn), replace with color from source
      if (grayData[idx] < 128) { // Black pixel in thresholded image
        finalDrawnData[idx] = colorData[idx];       // R
        finalDrawnData[idx + 1] = colorData[idx + 1]; // G
        finalDrawnData[idx + 2] = colorData[idx + 2]; // B
        finalDrawnData[idx + 3] = 255;                // A
      }
    }
    drawnCtx.putImageData(finalDrawnImageData, 0, 0);
    
    // Show the final drawn image (black lines replaced with color) for a moment
    const intermediateFrames = fps * 1; // 1 second
    for (let i = 0; i < intermediateFrames; i++) {
      recordCtx.clearRect(0, 0, w, h);
      recordCtx.drawImage(drawnCanvas, 0, 0);
      await new Promise((r) => setTimeout(r, Math.max(0, 1000 / fps)));
    }

    // Show final color image for a few seconds
    const finalFrames = fps * 2; // 2 seconds
    for (let i = 0; i < finalFrames; i++) {
      recordCtx.clearRect(0, 0, w, h);
      recordCtx.putImageData(sourceImageData, 0, 0);
      await new Promise((r) => setTimeout(r, Math.max(0, 1000 / fps)));
    }

    // Stop recording
    await new Promise((res) => {
      mediaRecorder.onstop = () => res();
      mediaRecorder.stop();
    });

    const superBuffer = new Blob(recordedBlobsRef.current, { type: "video/webm" });
    const url = window.URL.createObjectURL(superBuffer);
    setVideoUrl(url);
    
    setIsGenerating(false);
    setProgress(100);
  };

  /* -----------------------------
     Core drawing & generation logic (équivalent Python)
     ----------------------------- */

  const generateVideoFromImage = async (img, variables, handProc, recordCallback) => {
    const w = variables.resizeWd;
    const h = variables.resizeHt;

    // 1) preprocess: resize + grayscale + edge detection + adaptive threshold
    let imageData = resizeAndGetImageData(img, w, h);
    imageData = toGrayscale(imageData);
    imageData = applyEdgeDetection(imageData); // Add edge detection for better contour detection
    imageData = adaptiveThresholdApprox(imageData, 15, 10); // improved adaptive threshold

    // 2) Find and order strokes
    const strokes = findAndOrderStrokes(imageData, w, h);
    console.log(`Found ${strokes.length} strokes.`);

    // Keep a color-resized full image for final frames
    const colorCanvas = document.createElement("canvas");
    colorCanvas.width = w;
    colorCanvas.height = h;
    colorCanvas.getContext("2d").drawImage(img, 0, 0, w, h);
    const colorImageData = colorCanvas.getContext("2d").getImageData(0, 0, w, h);

    // Create drawn_frame (initially white)
    const drawnCanvas = document.createElement("canvas");
    drawnCanvas.width = w;
    drawnCanvas.height = h;
    const drawnCtx = drawnCanvas.getContext("2d");
    drawnCtx.fillStyle = "white";
    drawnCtx.fillRect(0, 0, w, h);

    // Prepare media recorder to record drawCanvasRef stream
    const recordCanvas = drawCanvasRef.current;
    recordCanvas.width = w;
    recordCanvas.height = h;
    const recordCtx = recordCanvas.getContext("2d");
    recordCtx.fillStyle = "white";
    recordCtx.fillRect(0, 0, w, h);

    recordedBlobsRef.current = [];
    setVideoUrl(null);

    const stream = recordCanvas.captureStream(variables.frameRate);
    const options = { mimeType: "video/webm;codecs=vp9" };
    let mediaRecorder;
    try {
      mediaRecorder = new MediaRecorder(stream, options);
    } catch {
      mediaRecorder = new MediaRecorder(stream, { mimeType: "video/webm;codecs=vp8" });
    }
    mediaRecorderRef.current = mediaRecorder;
    mediaRecorder.ondataavailable = (ev) => {
      if (ev.data && ev.data.size > 0) recordedBlobsRef.current.push(ev.data);
    };

    mediaRecorder.start();

    let currentPointIndex = 0;
    let currentStrokeIndex = 0;
    const totalPoints = strokes.flat().length; // Nombre total de pixels à dessiner
    let drawnPointsCount = 0;

    // Loop through strokes and points
    while (currentStrokeIndex < strokes.length) {
      const currentStroke = strokes[currentStrokeIndex];
      const traceSpeed = variables.strokeTraceSpeed; // Pixels tracés par "frame" ou étape

      for (let i = 0; i < traceSpeed; i++) {
        if (currentPointIndex >= currentStroke.length) {
          break; // Fin du stroke actuel
        }
        const [px, py] = currentStroke[currentPointIndex];

        // Draw the pixel on drawnCtx
        drawnCtx.fillStyle = "black"; // Tracer en noir
        drawnCtx.fillRect(px, py, 1, 1); // Dessine un pixel

        // Update hand position
        const handX = px;
        const handY = py;

        // Compose drawnCtx + hand on recordCanvas
        recordCtx.clearRect(0, 0, w, h); // Clear previous frame
        recordCtx.drawImage(drawnCanvas, 0, 0); // Draw what's been drawn so far

        // Draw hand overlay
        if (handProc) {
          const hw = handProc.width;
          const hh = handProc.height;
          // Ajuster la position de la main pour que la pointe du stylo soit sur (handX, handY)
          // Ceci nécessitera un ajustement précis basé sur la forme de la main/stylo
          // Pour l'instant, plaçons le centre du bas de la main sur le point
          // Cela pourrait être amélioré si la position de la pointe du stylo est connue dans l'image de la main
          const handDrawX = handX - handProc.width / 2; // Exemple: centrer la main sur le point
          const handDrawY = handY - handProc.height; // Exemple: positionner le bas de la main sur le point

          recordCtx.drawImage(
            handProc.canvas,
            Math.max(0, -handDrawX), Math.max(0, -handDrawY), // Source clipping
            Math.min(hw, w - handDrawX, handDrawX + hw), Math.min(hh, h - handDrawY, handDrawY + hh), // Source clipping width/height
            handDrawX, handDrawY, // Destination X, Y
            Math.min(hw, w - handDrawX), Math.min(hh, h - handDrawY) // Destination W, H
          );
        }

        drawnPointsCount++;
        currentPointIndex++;
      }

      // If current stroke is finished, move to the next
      if (currentPointIndex >= currentStroke.length) {
        currentStrokeIndex++;
        currentPointIndex = 0; // Reset point index for next stroke
      }

      // Update progress
      setProgress(Math.floor((drawnPointsCount / totalPoints) * 100));

      // Wait for next animation frame
      await new Promise((r) => setTimeout(r, Math.max(0, 1000 / variables.frameRate)));
    }

    // Final frame: remove hand, show full drawing
    recordCtx.clearRect(0, 0, w, h);
    recordCtx.drawImage(drawnCanvas, 0, 0);
    await new Promise((r) => setTimeout(r, Math.max(0, 1000 / variables.frameRate)));


    // draw final color image for a few seconds to match python behaviour
    const showFinalFrames = variables.frameRate * variables.endGrayImgDurationInSec;
    for (let i = 0; i < showFinalFrames; i++) {
      recordCtx.clearRect(0, 0, w, h);
      recordCtx.putImageData(colorImageData, 0, 0);
      await new Promise((r) => setTimeout(r, Math.max(0, 1000 / variables.frameRate)));
    }

    // stop recorder and produce blob URL
    await new Promise((res) => {
      mediaRecorder.onstop = () => res();
      mediaRecorder.stop();
    });

    const superBuffer = new Blob(recordedBlobsRef.current, { type: "video/webm" });
    const url = window.URL.createObjectURL(superBuffer);
    setVideoUrl(url);

    if (recordCallback) recordCallback(url);
    setIsGenerating(false);
    setProgress(100);
  };

  /* -----------------------------
     Handlers & UI
     ----------------------------- */

  const handleSourceUpload = (e) => {
    const file = e.target.files[0];
    if (!file) return;
    
    const reader = new FileReader();
    reader.onload = (event) => {
      setPendingImageUrl(event.target.result);
      setCropTargetMode("image");
      setShowCropModal(true);
    };
    reader.readAsDataURL(file);
    e.target.value = '';
  };

  const handleCropComplete = (croppedImageUrl) => {
    const img = new Image();
    img.onload = () => {
      if (cropTargetMode === "image") {
        setSourceImage(img);
        const ctx = sourceCanvasRef.current.getContext("2d");
        ctx.clearRect(0, 0, sourceCanvasRef.current.width, sourceCanvasRef.current.height);
        ctx.drawImage(img, 0, 0, img.width, img.height, 0, 0, sourceCanvasRef.current.width, sourceCanvasRef.current.height);
      } else if (cropTargetMode === "json") {
        setSourceImageForJson(img);
        const ctx = sourceCanvasRef.current.getContext("2d");
        ctx.clearRect(0, 0, sourceCanvasRef.current.width, sourceCanvasRef.current.height);
        ctx.drawImage(img, 0, 0, img.width, img.height, 0, 0, sourceCanvasRef.current.width, sourceCanvasRef.current.height);
      }
    };
    img.src = croppedImageUrl;
    setShowCropModal(false);
    setPendingImageUrl(null);
    setCropTargetMode(null);
  };

  const handleCropCancel = () => {
    setShowCropModal(false);
    setPendingImageUrl(null);
    setCropTargetMode(null);
  };

  const handleStart = async () => {
    if (mode === "image") {
      // Original image generation mode
      if (!sourceImage) {
        alert("Upload an image first");
        return;
      }
      setIsGenerating(true);
      setProgress(0);
      setVideoUrl(null);

      const vars = { ...variablesDefault };
      const handProc = await preprocessHand(handImage, handMask);

      await generateVideoFromImage(sourceImage, vars, handProc, (url) => {
        console.log("video ready", url);
      });
    } else if (mode === "json") {
      // JSON replay mode
      if (!animationData) {
        alert("Upload a JSON animation file first");
        return;
      }
      if (!sourceImageForJson) {
        alert("Upload the source image that was used to generate the JSON");
        return;
      }
      setIsGenerating(true);
      setProgress(0);
      setVideoUrl(null);

      const handProc = await preprocessHand(handImage, handMask);

      await generateVideoFromJson(animationData, sourceImageForJson, handProc);
    }
  };

  const handleDownload = () => {
    if (!videoUrl) {
      alert("Génère la vidéo d'abord");
      return;
    }
    const a = document.createElement("a");
    a.href = videoUrl;
    a.download = "handwriting.webm";
    a.click();
  };

  return (
    <div className="p-6 bg-white min-h-screen">
      {/* Image Crop Modal */}
      {showCropModal && pendingImageUrl && (
        <ImageCropModal
          imageUrl={pendingImageUrl}
          onCropComplete={handleCropComplete}
          onCancel={handleCropCancel}
        />
      )}
      
      <Card className="max-w-4xl mx-auto bg-secondary/30 border-border">
        <CardHeader>
          <CardTitle className="text-white">Handwriting Animation Generator</CardTitle>
          <p className="text-muted-foreground text-sm">
            Générez des animations à partir d&apos;images ou rejouez des animations JSON (exportées depuis Python)
          </p>
        </CardHeader>
        <CardContent className="space-y-4">
          {/* Mode Selector */}
          <div className="flex gap-2 p-2 bg-secondary rounded-lg">
            <Button
              variant={mode === "image" ? "default" : "outline"}
              onClick={() => setMode("image")}
              className="flex-1"
            >
              <Upload className="w-4 h-4 mr-2" />
              Mode Image
            </Button>
            <Button
              variant={mode === "json" ? "default" : "outline"}
              onClick={() => setMode("json")}
              className="flex-1"
            >
              <FileJson className="w-4 h-4 mr-2" />
              Mode JSON
            </Button>
          </div>

          {/* Image Mode Controls */}
          {mode === "image" && (
            <div className="flex gap-4 items-center">
              <Button
                variant="outline"
                onClick={() => document.getElementById("src-upload").click()}
                className="flex items-center gap-2"
              >
                <Upload className="w-4 h-4" />
                Upload Image
              </Button>
              <input id="src-upload" onChange={handleSourceUpload} className="hidden" type="file" accept="image/*" />
              <Button onClick={handleStart} disabled={!sourceImage || isGenerating} className="flex items-center gap-2">
                <Play className="w-4 h-4" />
                {isGenerating ? "Génération..." : "Générer"}
              </Button>
              <Button variant="outline" onClick={handleDownload} disabled={!videoUrl} className="flex items-center gap-2">
                <Download className="w-4 h-4" />
                Download
              </Button>
            </div>
          )}

          {/* JSON Mode Controls */}
          {mode === "json" && (
            <div className="space-y-4">
              <div className="flex gap-4 items-center">
                <Button
                  variant="outline"
                  onClick={() => document.getElementById("json-upload").click()}
                  className="flex items-center gap-2"
                >
                  <FileJson className="w-4 h-4" />
                  Upload JSON
                </Button>
                <input id="json-upload" onChange={handleJsonUpload} className="hidden" type="file" accept="application/json,.json" />
                
                <Button
                  variant="outline"
                  onClick={() => document.getElementById("json-img-upload").click()}
                  className="flex items-center gap-2"
                  disabled={!animationData}
                >
                  <Upload className="w-4 h-4" />
                  Upload Source Image
                </Button>
                <input id="json-img-upload" onChange={handleSourceImageForJsonUpload} className="hidden" type="file" accept="image/*" />
                
                <Button onClick={handleStart} disabled={!animationData || !sourceImageForJson || isGenerating} className="flex items-center gap-2">
                  <Play className="w-4 h-4" />
                  {isGenerating ? "Lecture..." : "Rejouer"}
                </Button>
                <Button variant="outline" onClick={handleDownload} disabled={!videoUrl} className="flex items-center gap-2">
                  <Download className="w-4 h-4" />
                  Download
                </Button>
              </div>
              
              {animationData && (
                <div className="bg-secondary p-3 rounded text-sm text-foreground">
                  <p><strong>Animation chargée:</strong></p>
                  <p>• Résolution: {animationData.metadata.width}x{animationData.metadata.height}</p>
                  <p>• FPS: {animationData.metadata.frame_rate}</p>
                  <p>• Frames: {animationData.metadata.total_frames}</p>
                  <p>• Grille: {animationData.metadata.split_len}px</p>
                </div>
              )}
            </div>
          )}

          {isGenerating && (
            <div className="w-full bg-secondary rounded-full h-2">
              <div className="bg-primary h-2 rounded-full transition-all" style={{ width: `${progress}%` }} />
            </div>
          )}
          <p className="text-xs text-muted-foreground">Progress: {progress}%</p>

          <div className="grid grid-cols-2 gap-4">
            <div>
              <h3 className="text-white text-sm mb-2">Source Preview</h3>
              <canvas ref={sourceCanvasRef} width={variablesDefault.resizeWd} height={variablesDefault.resizeHt} className="border border-border bg-white w-full" />
            </div>
            <div>
              <h3 className="text-white text-sm mb-2">Recorded Canvas (video frames)</h3>
              <canvas ref={drawCanvasRef} width={variablesDefault.resizeWd} height={variablesDefault.resizeHt} className="border border-border bg-white w-full" />
            </div>
          </div>

          <div className="text-sm text-muted-foreground">
            <p>Hand Image: {handImage ? "✓ Loaded" : "⏳ Loading..."}</p>
            <p>Hand Mask: {handMask ? "✓ Loaded" : "⏳ Loading..."}</p>
            {videoUrl && (
              <p className="mt-2">
                Vidéo générée : <a href={videoUrl} target="_blank" rel="noreferrer" className="text-primary underline">Voir</a>
              </p>
            )}
          </div>
        </CardContent>
      </Card>
    </div>
  );
};

export default HandWritingAnimation;